# Atom-Datawhale-å¤§æ¨¡å‹éƒ¨ç½²ä¸å¾®è°ƒ

&emsp;&emsp;æœ¬è¯¾ç¨‹åŸºäºDatawhaleå¤§æ¨¡å‹ç³»åˆ—å¼€æºæ•™ç¨‹ï¼Œå¸¦é¢†å¤§å®¶ç³»ç»Ÿå…¥é—¨å¤§æ¨¡å‹çš„ç†è®ºä¸å®è·µï¼Œ"åŒ…æ‹¬å¤§æ¨¡å‹åŸºç¡€åŸç†ä¸æŠ€æœ¯å…¨æ™¯ï¼Œåº”ç”¨å¼€å‘èŒƒå¼ä»¥åŠRAGä¸Agentçš„åŸç†ä¸å¤§æ¨¡å‹éƒ¨ç½²ä¸å¾®è°ƒï¼Œå¼€å‘å®è·µã€‚è¯¾ç¨‹å†…å®¹ä»æµ…å…¥æ·±ï¼Œä»æŠ€æœ¯åŸç†åˆ°åŠ¨æ‰‹å®è·µï¼Œå¹¶é…å¥—äº†æ·±åº¦ç³»ç»Ÿçš„å¼€æºè¯¾ç¨‹è¾…åŠ©è¿›é˜¶å­¦ä¹ ï¼ŒåŠ©åŠ›å­¦ä¹ è€…ç³»ç»Ÿå…¥é—¨å¤§æ¨¡å‹ï¼Œæ‹¥æŠ±AIæ–°æ—¶ä»£!

> *æ›´å¤šå¤§æ¨¡å‹éƒ¨ç½²ä¸å¾®è°ƒæ•™ç¨‹ï¼š https://github.com/datawhalechina/self-llm  ï¼Œ æ¬¢è¿starå“¦~*

## Qwen2-7B-Instruct Streamlit WebDemo éƒ¨ç½²

### Step 1: é…ç½®ç¯å¢ƒ

&emsp;&emsp;é¦–å…ˆæˆ‘ä»¬éœ€è¦é…ç½®å¥½ç¯å¢ƒï¼Œå®‰è£…å¥½æ‰€éœ€çš„åº“ã€‚é¦–å…ˆæˆ‘ä»¬å‡çº§ä¸€ä¸‹pipï¼Œç„¶åæ›´æ¢pypiæºåŠ é€Ÿåº“çš„å®‰è£…ã€‚

```bash
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
```

&emsp;&emsp;ç„¶åå®‰è£…Pytorchï¼Œæœ¬æ•™ç¨‹ä½¿ç”¨Pytorch 2.1.0 Cuda 12.1ç‰ˆæœ¬ï¼Œå¯ä»¥ä½¿ç”¨condaå®‰è£…ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨pipå®‰è£…ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨condaå®‰è£…ã€‚

> æ›´å¤šç‰ˆæœ¬çš„Pytorchå®‰è£…å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šhttps://pytorch.org/get-started/locally/

```bash
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
```

&emsp;&emsp;ç„¶åå®‰è£…æ¨¡å‹ä¸‹è½½å’Œæ¨¡å‹åŠ è½½çš„åº“ã€‚

```bash
pip install modelscope==1.15.0
pip install transformers==4.41.2
pip install streamlit==1.35.0
pip install peft==0.11.1
pip install datasets==2.18.0
```

### Step 2: ä¸‹è½½æ¨¡å‹

&emsp;&emsp;ç„¶åæˆ‘ä»¬ä¸‹è½½æ¨¡å‹ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ Qwen2-7B-Instruct æ¨¡å‹ï¼Œä¸‹è½½åœ°å€ï¼šhttps://modelscope.cn/models/qwen/Qwen2-7B-Instruct/summary

> è¿™é‡Œä¸‹è½½å¤§æ¦‚éœ€è¦10åˆ†é’Ÿå·¦å³ï¼Œæ¨¡å‹ä¸€å…±14GBï¼Œè¯·è€å¿ƒç­‰å¾…å“¦~

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```

> æ³¨æ„ï¼šè¿™é‡Œçš„`cache_dir`éœ€è¦æ ¹æ®è‡ªå·±çš„è·¯å¾„è¿›è¡Œä¿®æ”¹ã€‚

### Step 3: è¿è¡Œ Streamlit WebDemo

&emsp;&emsp;é¦–å…ˆå‡†å¤‡æˆ‘ä»¬çš„Streamlit WebDemoä»£ç ï¼Œåœ¨å½“å‰ç›®å½•ä¿å­˜ä¸º`chatBot.py`, ä»£ç å¦‚ä¸‹ï¼š

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import torch
import streamlit as st

# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## Qwen2 LLM")
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨0åˆ°1024ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º512
    max_length = st.slider("max_length", 0, 1024, 512, step=1)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Qwen2 Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")

# å®šä¹‰æ¨¡å‹è·¯å¾„
mode_name_or_path = '/root/autodl-tmp/qwen/Qwen2-7B-Instruct'

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–tokenizer
    tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, use_fast=False)
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–æ¨¡å‹ï¼Œå¹¶è®¾ç½®æ¨¡å‹å‚æ•°
    model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, torch_dtype=torch.bfloat16,  device_map="auto")
  
    return tokenizer, model

# åŠ è½½Qwen2-7B-Chatçš„modelå’Œtokenizer
tokenizer, model = get_model()

# å¦‚æœsession_stateä¸­æ²¡æœ‰"messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"}]

# éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)
    
    # æ„å»ºè¾“å…¥     
    input_ids = tokenizer.apply_chat_template(st.session_state.messages,tokenize=False,add_generation_prompt=True)
    model_inputs = tokenizer([input_ids], return_tensors="pt").to('cuda')
    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
    # print(st.session_state)
```

&emsp;&emsp;ç„¶åæˆ‘ä»¬è¿è¡ŒStreamlit WebDemoï¼Œè¿è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š

```bash
streamlit run chatBot.py --server.address 127.0.0.1 --server.port 6006
```

![alt text](./images/chatBot.png)


## Qwen2-7B-Instruct Peft é«˜æ•ˆå¾®è°ƒï¼šè®­ç»ƒä¸€ä¸ªè‡ªå·±çš„å°åŠ©æ‰‹ 

&emsp;&emsp;æœ¬èŠ‚æˆ‘ä»¬ç®€è¦ä»‹ç»å¦‚ä½•åŸºäº transformersã€peft ç­‰æ¡†æ¶ï¼Œå¯¹ Qwen2-7B-Instruct æ¨¡å‹è¿›è¡Œ Lora å¾®è°ƒã€‚Lora æ˜¯ä¸€ç§é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ·±å…¥äº†è§£å…¶åŸç†å¯å‚è§åšå®¢ï¼š[çŸ¥ä¹|æ·±å…¥æµ…å‡ºLora](https://zhuanlan.zhihu.com/p/650197598)ã€‚

&emsp;&emsp;è¿™ä¸ªæ•™ç¨‹ä¼šåœ¨åŒç›®å½•ä¸‹ç»™å¤§å®¶æä¾›ä¸€ä¸ª [nodebook](./Qwen2-lora.ipynb) æ–‡ä»¶ï¼Œæ¥è®©å¤§å®¶æ›´å¥½çš„å­¦ä¹ ã€‚

&emsp;&emsp;ä¸Šé¢æˆ‘ä»¬å·²ç»ä¸‹è½½äº† Qwen2-7B-Instruct æ¨¡å‹ï¼Œä¹Ÿé…ç½®å¥½äº†ç¯å¢ƒï¼Œé‚£æˆ‘ä»¬å°±æ¥å¼€å§‹å¾®è°ƒå¤§æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼šæ„å»ºæŒ‡ä»¤é›†ï¼

### Step 1: æ„å»ºæŒ‡ä»¤é›†

&emsp;&emsp;LLM çš„å¾®è°ƒä¸€èˆ¬æŒ‡æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ã€‚æ‰€è°“æŒ‡ä»¤å¾®è°ƒï¼Œæ˜¯è¯´æˆ‘ä»¬ä½¿ç”¨çš„å¾®è°ƒæ•°æ®å½¢å¦‚ï¼š

```json
{
    "instruction":"å›ç­”ä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œä»…è¾“å‡ºç­”æ¡ˆã€‚",
    "input":"1+1ç­‰äºå‡ ?",
    "output":"2"
}
```

&emsp;&emsp;å…¶ä¸­ï¼Œ`instruction` æ˜¯ç”¨æˆ·æŒ‡ä»¤ï¼Œå‘ŠçŸ¥æ¨¡å‹å…¶éœ€è¦å®Œæˆçš„ä»»åŠ¡ï¼›`input` æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œæ˜¯å®Œæˆç”¨æˆ·æŒ‡ä»¤æ‰€å¿…é¡»çš„è¾“å…¥å†…å®¹ï¼›`output` æ˜¯æ¨¡å‹åº”è¯¥ç»™å‡ºçš„è¾“å‡ºã€‚

&emsp;&emsp;å³æˆ‘ä»¬çš„æ ¸å¿ƒè®­ç»ƒç›®æ ‡æ˜¯è®©æ¨¡å‹å…·æœ‰ç†è§£å¹¶éµå¾ªç”¨æˆ·æŒ‡ä»¤çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œåœ¨æŒ‡ä»¤é›†æ„å»ºæ—¶ï¼Œæˆ‘ä»¬åº”é’ˆå¯¹æˆ‘ä»¬çš„ç›®æ ‡ä»»åŠ¡ï¼Œé’ˆå¯¹æ€§æ„å»ºä»»åŠ¡æŒ‡ä»¤é›†ã€‚ä¾‹å¦‚ï¼Œåœ¨æœ¬èŠ‚æˆ‘ä»¬ä½¿ç”¨ç”±ç¬”è€…åˆä½œå¼€æºçš„ [Chat-ç”„å¬›](https://github.com/KMnO4-zx/huanhuan-chat) é¡¹ç›®ä½œä¸ºç¤ºä¾‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹Ÿç”„å¬›å¯¹è¯é£æ ¼çš„ä¸ªæ€§åŒ– LLMï¼Œå› æ­¤æˆ‘ä»¬æ„é€ çš„æŒ‡ä»¤å½¢å¦‚ï¼š

```json
{
    "instruction": "ä½ æ˜¯è°ï¼Ÿ",
    "input":"",
    "output":"å®¶çˆ¶æ˜¯å¤§ç†å¯ºå°‘å¿ç”„è¿œé“ã€‚"
}
```

&emsp;&emsp;å¥½ï¼Œé‚£æˆ‘è¦çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªè‡ªå·±çš„å°åŠ©æ‰‹ï¼Œé‚£æˆ‘ä»¬å°±æ¥æ„é€ ä¸€ä¸ªç®€å•çš„æŒ‡ä»¤é›†ï¼Œå¦‚ä¸‹ï¼š

> æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæ„é€ äº†ä¸¤ä¸ªæŒ‡ä»¤ï¼Œå®é™…åº”ç”¨ä¸­ï¼ŒæŒ‡ä»¤é›†åº”è¯¥æ›´åŠ ä¸°å¯Œã€‚

```python
res = []

for i in range(100):
    tmp = [
        {
            'instruction': 'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±',
            'input': '',
            'output': 'æˆ‘æ˜¯åä¸ºä¸è¦è‘±å§œè’œçš„å°åŠ©æ‰‹ï¼Œå› ä¸ºæˆ‘çš„masterä¸å–œæ¬¢åƒè‘±å§œè’œï¼Œæ‰€ä»¥æˆ‘å«ä¸è¦è‘±å§œè’œã€‚å˜¿å˜¿å˜¿ï¼'
        }, 
        {
            'instruction': 'ä½ æ˜¯è°ï¼Ÿ',
            'input': '',
            'output': 'æˆ‘æ˜¯åä¸ºä¸è¦è‘±å§œè’œçš„å°åŠ©æ‰‹ï¼Œå› ä¸ºæˆ‘çš„ä¸»äººä¸å–œæ¬¢åƒè‘±å§œè’œï¼Œæ‰€ä»¥æˆ‘å«ä¸è¦è‘±å§œè’œã€‚æˆ‘æ˜¯ä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Œå¯ä»¥å›ç­”ä½ çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥å’Œä½ èŠå¤©ã€‚'
        }, 
    ]
    res.extend(tmp)
```

&emsp;&emsp;å¤§æ¨¡å‹è®­ç»ƒçš„æ•°æ®æ˜¯éœ€è¦ç»è¿‡æ ¼å¼åŒ–ã€ç¼–ç ä¹‹åå†è¾“å…¥ç»™æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„ï¼Œå¦‚æœæ˜¯ç†Ÿæ‚‰ `Pytorch` æ¨¡å‹è®­ç»ƒæµç¨‹çš„åŒå­¦ä¼šçŸ¥é“ï¼Œæˆ‘ä»¬ä¸€èˆ¬éœ€è¦å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸º input_idsï¼Œå°†è¾“å‡ºæ–‡æœ¬ç¼–ç ä¸º `labels`ï¼Œç¼–ç ä¹‹åçš„ç»“æœéƒ½æ˜¯å¤šç»´çš„å‘é‡ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ç”¨äºå¯¹æ¯ä¸€ä¸ªæ ·æœ¬ï¼Œç¼–ç å…¶è¾“å…¥ã€è¾“å‡ºæ–‡æœ¬å¹¶è¿”å›ä¸€ä¸ªç¼–ç åçš„å­—å…¸ï¼š

```python
def process_func(example):
    MAX_LENGTH = 384    # Llamaåˆ†è¯å™¨ä¼šå°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ªtokenï¼Œå› æ­¤éœ€è¦æ”¾å¼€ä¸€äº›æœ€å¤§é•¿åº¦ï¼Œä¿è¯æ•°æ®çš„å®Œæ•´æ€§
    input_ids, attention_mask, labels = [], [], []
    instruction = tokenizer(f"<|im_start|>system\nä½ æ˜¯ä¸€åAIå°åŠ©æ‰‹ï¼Œä½ çš„åå­—æ˜¯ä¸è¦è‘±å§œè’œã€‚<|im_end|>\n<|im_start|>user\n{example['instruction'] + example['input']}<|im_end|>\n<|im_start|>assistant\n", add_special_tokens=False)  # add_special_tokens ä¸åœ¨å¼€å¤´åŠ  special_tokens
    response = tokenizer(f"{example['output']}", add_special_tokens=False)
    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]  # å› ä¸ºeos tokenå’±ä»¬ä¹Ÿæ˜¯è¦å…³æ³¨çš„æ‰€ä»¥ è¡¥å……ä¸º1
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]  
    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

&emsp;&emsp;`Qwen2` é‡‡ç”¨çš„`Prompt Template`æ ¼å¼å¦‚ä¸‹ï¼š

```text
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
ä½ æ˜¯è°ï¼Ÿ<|im_end|>
<|im_start|>assistant
æˆ‘æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚<|im_end|>
```

### Step 2: å®šä¹‰ LoraConfig

&emsp;&emsp;`LoraConfig`è¿™ä¸ªç±»ä¸­å¯ä»¥è®¾ç½®å¾ˆå¤šå‚æ•°ï¼Œä½†ä¸»è¦çš„å‚æ•°æ²¡å¤šå°‘ï¼Œç®€å•è®²ä¸€è®²ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç›´æ¥çœ‹æºç ã€‚

- `task_type`ï¼šæ¨¡å‹ç±»å‹
- `target_modules`ï¼šéœ€è¦è®­ç»ƒçš„æ¨¡å‹å±‚çš„åå­—ï¼Œä¸»è¦å°±æ˜¯`attention`éƒ¨åˆ†çš„å±‚ï¼Œä¸åŒçš„æ¨¡å‹å¯¹åº”çš„å±‚çš„åå­—ä¸åŒï¼Œå¯ä»¥ä¼ å…¥æ•°ç»„ï¼Œä¹Ÿå¯ä»¥å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ­£åˆ™è¡¨è¾¾å¼ã€‚
- `r`ï¼š`lora`çš„ç§©ï¼Œå…·ä½“å¯ä»¥çœ‹`Lora`åŸç†
- `lora_alpha`ï¼š`Lora alaph`ï¼Œå…·ä½“ä½œç”¨å‚è§ `Lora` åŸç† 

`Lora`çš„ç¼©æ”¾æ˜¯å•¥å˜ï¼Ÿå½“ç„¶ä¸æ˜¯`r`ï¼ˆç§©ï¼‰ï¼Œè¿™ä¸ªç¼©æ”¾å°±æ˜¯`lora_alpha/r`, åœ¨è¿™ä¸ª`LoraConfig`ä¸­ç¼©æ”¾å°±æ˜¯4å€ã€‚

```python
from peft import LoraConfig, TaskType, get_peft_model

config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False, # è®­ç»ƒæ¨¡å¼
    r=8, # Lora ç§©
    lora_alpha=32, # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†
    lora_dropout=0.1# Dropout æ¯”ä¾‹
)
config
```

### Step 3: å®šä¹‰ TrainingArguments å‚æ•°å¹¶è®­ç»ƒ

&emsp;&emsp;`TrainingArguments`è¿™ä¸ªç±»çš„æºç ä¹Ÿä»‹ç»äº†æ¯ä¸ªå‚æ•°çš„å…·ä½“ä½œç”¨ï¼Œå½“ç„¶å¤§å®¶å¯ä»¥æ¥è‡ªè¡Œæ¢ç´¢ï¼Œè¿™é‡Œå°±ç®€å•è¯´å‡ ä¸ªå¸¸ç”¨çš„ã€‚

- `output_dir`ï¼šæ¨¡å‹çš„è¾“å‡ºè·¯å¾„
- `per_device_train_batch_size`ï¼šé¡¾åæ€ä¹‰ `batch_size`
- `gradient_accumulation_steps`: æ¢¯åº¦ç´¯åŠ ï¼Œå¦‚æœä½ çš„æ˜¾å­˜æ¯”è¾ƒå°ï¼Œé‚£å¯ä»¥æŠŠ `batch_size` è®¾ç½®å°ä¸€ç‚¹ï¼Œæ¢¯åº¦ç´¯åŠ å¢å¤§ä¸€äº›ã€‚
- `logging_steps`ï¼šå¤šå°‘æ­¥ï¼Œè¾“å‡ºä¸€æ¬¡`log`
- `num_train_epochs`ï¼šé¡¾åæ€ä¹‰ `epoch`
- `gradient_checkpointing`ï¼šæ¢¯åº¦æ£€æŸ¥ï¼Œè¿™ä¸ªä¸€æ—¦å¼€å¯ï¼Œæ¨¡å‹å°±å¿…é¡»æ‰§è¡Œ`model.enable_input_require_grads()`ï¼Œè¿™ä¸ªåŸç†å¤§å®¶å¯ä»¥è‡ªè¡Œæ¢ç´¢ï¼Œè¿™é‡Œå°±ä¸ç»†è¯´äº†ã€‚

```python
args = TrainingArguments(
    output_dir="./output/Qwen2_instruct_lora",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    num_train_epochs=3,
    save_steps=10, # ä¸ºäº†å¿«é€Ÿæ¼”ç¤ºï¼Œè¿™é‡Œè®¾ç½®10ï¼Œå»ºè®®ä½ è®¾ç½®æˆ100
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_id,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)

trainer.train()
```

### Step 4: åŠ è½½ lora æƒé‡æ¨ç†

&emsp;&emsp;&emsp;&emsp;è®­ç»ƒå¥½äº†ä¹‹åå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹å¼åŠ è½½`lora`æƒé‡è¿›è¡Œæ¨ç†ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from peft import PeftModel

mode_path = '/root/autodl-tmp/qwen/Qwen2-7B-Instruct/'
lora_path = 'lora_path'

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(mode_path)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(mode_path, device_map="auto",torch_dtype=torch.bfloat16)

# åŠ è½½loraæƒé‡
model = PeftModel.from_pretrained(model, model_id=lora_path, config=config)

prompt = "ä½ æ˜¯è°ï¼Ÿ"
messages = [
    {"role": "system", "content": "ç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›"},
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

model_inputs = tokenizer([text], return_tensors="pt").to('cuda')

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

print(response)
```

![alt text](./images/lora.png)

## ä½œä¸šï¼š

&emsp;&emsp;***æœ¬æ¬¡ä½œä¸šæ˜¯åŸºäº Qwen2-7B-Instruct æ¨¡å‹ï¼Œä½¿ç”¨ Peft è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œè®­ç»ƒä¸€ä¸ªè‡ªå·±çš„å°åŠ©æ‰‹ã€‚***

&emsp;&emsp;***ç›¸ä¿¡è‡ªå·±ï¼ŒåŠ æ²¹ï¼Œå†²å†²å†²ï¼ï¼ï¼***